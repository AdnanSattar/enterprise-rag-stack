{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Engineering Demo Notebook\n",
        "\n",
        "A complete, runnable demonstration of production RAG patterns:\n",
        "- Document ingestion and normalization\n",
        "- Semantic chunking\n",
        "- Embedding generation\n",
        "- Hybrid retrieval (vector + lexical)\n",
        "- Reranking\n",
        "- Context assembly\n",
        "- Answer verification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Install dependencies (uncomment for Colab):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install chromadb sentence-transformers tiktoken numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Dict\n",
        "import hashlib\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Sample Corpus\n",
        "\n",
        "Enterprise documents for demonstration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = [\n",
        "    {\n",
        "        \"doc_id\": \"contract_001\",\n",
        "        \"title\": \"Payment Terms\",\n",
        "        \"text\": \"\"\"Payment Terms and Conditions\n",
        "\n",
        "The customer must pay within 30 days of invoice date. Late fees apply after 45 days at a rate of 1.5% per month.\n",
        "\n",
        "Payment Methods:\n",
        "- Wire transfer (preferred)\n",
        "- ACH direct debit\n",
        "- Credit card (3% processing fee applies)\n",
        "\n",
        "Early payment discount: 2% discount for payment within 10 days.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"doc_id\": \"policy_legal_01\",\n",
        "        \"title\": \"Data Retention Policy\",\n",
        "        \"text\": \"\"\"Data Retention Policy\n",
        "\n",
        "Customer data is retained for 7 years for audit and compliance purposes. This includes:\n",
        "- Transaction records\n",
        "- Communication logs\n",
        "- Account information\n",
        "\n",
        "Data can be deleted upon written request, subject to legal retention requirements.\n",
        "Backup data is retained for an additional 90 days after primary deletion.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"doc_id\": \"faq_01\",\n",
        "        \"title\": \"Password Reset FAQ\",\n",
        "        \"text\": \"\"\"Frequently Asked Questions - Account Security\n",
        "\n",
        "Q: How can I reset my password?\n",
        "A: Use the self-service portal at /reset. You will receive a verification email within 5 minutes.\n",
        "\n",
        "Q: What are the password requirements?\n",
        "A: Minimum 12 characters, including uppercase, lowercase, number, and special character.\n",
        "\n",
        "Q: How often should I change my password?\n",
        "A: We recommend changing passwords every 90 days.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"doc_id\": \"sla_001\",\n",
        "        \"title\": \"Service Level Agreement\",\n",
        "        \"text\": \"\"\"Service Level Agreement (SLA)\n",
        "\n",
        "Uptime Guarantee: 99.9% availability measured monthly.\n",
        "\n",
        "Response Times:\n",
        "- Critical issues: 15 minutes initial response\n",
        "- High priority: 1 hour initial response\n",
        "- Medium priority: 4 hours initial response\n",
        "- Low priority: 24 hours initial response\n",
        "\n",
        "Service Credits:\n",
        "- Below 99.9%: 10% credit\n",
        "- Below 99.5%: 25% credit\n",
        "- Below 99.0%: 50% credit\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"doc_id\": \"product_spec_001\",\n",
        "        \"title\": \"API Rate Limits\",\n",
        "        \"text\": \"\"\"API Rate Limits and Quotas\n",
        "\n",
        "Standard Tier:\n",
        "- 1000 requests per minute\n",
        "- 100,000 requests per day\n",
        "- Maximum payload: 10MB\n",
        "\n",
        "Enterprise Tier:\n",
        "- 10,000 requests per minute\n",
        "- Unlimited daily requests\n",
        "- Maximum payload: 100MB\n",
        "\n",
        "Rate limit headers are included in all responses: X-RateLimit-Remaining, X-RateLimit-Reset.\"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "print(f\"Loaded {len(docs)} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Document Normalization\n",
        "\n",
        "Clean input beats clever retrieval:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BOILERPLATE_PATTERNS = [\n",
        "    r\"(?i)confidential\\s+information\",\n",
        "    r\"(?i)all\\s+rights\\s+reserved\",\n",
        "    r\"(?i)page\\s+\\d+\\s+of\\s+\\d+\",\n",
        "]\n",
        "\n",
        "def strip_boilerplate(text: str) -> str:\n",
        "    \"\"\"Remove common boilerplate text.\"\"\"\n",
        "    for pat in BOILERPLATE_PATTERNS:\n",
        "        text = re.sub(pat, \" \", text)\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "    return text.strip()\n",
        "\n",
        "def normalize_document(doc: dict, tenant_id: str = \"demo\") -> dict:\n",
        "    \"\"\"Full document normalization pipeline.\"\"\"\n",
        "    cleaned = strip_boilerplate(doc[\"text\"])\n",
        "    doc_hash = hashlib.md5(cleaned.encode()).hexdigest()\n",
        "    \n",
        "    return {\n",
        "        \"doc_id\": doc[\"doc_id\"],\n",
        "        \"title\": doc.get(\"title\", \"\"),\n",
        "        \"text\": cleaned,\n",
        "        \"metadata\": {\n",
        "            \"content_hash\": doc_hash,\n",
        "            \"tenant_id\": tenant_id,\n",
        "            \"ingested_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "            \"char_count\": len(cleaned),\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Normalize all documents\n",
        "normalized_docs = [normalize_document(doc) for doc in docs]\n",
        "print(f\"Normalized {len(normalized_docs)} documents\")\n",
        "print(f\"Sample metadata: {normalized_docs[0]['metadata']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Semantic Chunking\n",
        "\n",
        "Chunking determines what the retriever can find:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "def num_tokens(text: str) -> int:\n",
        "    \"\"\"Count tokens using cl100k_base encoding.\"\"\"\n",
        "    return len(enc.encode(text))\n",
        "\n",
        "def semantic_chunk(text: str, max_tokens: int = 300, overlap_tokens: int = 50) -> List[str]:\n",
        "    \"\"\"\n",
        "    Semantic chunking with overlap.\n",
        "    \n",
        "    Rules:\n",
        "    - Aim for 150-300 words per chunk\n",
        "    - Use 5-15% overlap\n",
        "    - Preserve structure (headings, lists)\n",
        "    \"\"\"\n",
        "    if num_tokens(text) <= max_tokens:\n",
        "        return [text]\n",
        "    \n",
        "    paragraphs = text.split(\"\\n\\n\")\n",
        "    chunks = []\n",
        "    current = []\n",
        "    current_tokens = 0\n",
        "    \n",
        "    for para in paragraphs:\n",
        "        para_tokens = num_tokens(para)\n",
        "        \n",
        "        if current_tokens + para_tokens > max_tokens and current:\n",
        "            chunks.append(\"\\n\\n\".join(current))\n",
        "            # Keep overlap\n",
        "            overlap = []\n",
        "            overlap_tok = 0\n",
        "            for p in reversed(current):\n",
        "                overlap.insert(0, p)\n",
        "                overlap_tok += num_tokens(p)\n",
        "                if overlap_tok >= overlap_tokens:\n",
        "                    break\n",
        "            current = overlap\n",
        "            current_tokens = sum(num_tokens(p) for p in current)\n",
        "        \n",
        "        current.append(para)\n",
        "        current_tokens += para_tokens\n",
        "    \n",
        "    if current:\n",
        "        chunks.append(\"\\n\\n\".join(current))\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Chunk all documents\n",
        "all_chunks = []\n",
        "for doc in normalized_docs:\n",
        "    text_chunks = semantic_chunk(doc[\"text\"])\n",
        "    for i, chunk_text in enumerate(text_chunks):\n",
        "        all_chunks.append({\n",
        "            \"id\": f\"{doc['doc_id']}#chunk_{i}\",\n",
        "            \"doc_id\": doc[\"doc_id\"],\n",
        "            \"title\": doc[\"title\"],\n",
        "            \"text\": chunk_text,\n",
        "            \"token_count\": num_tokens(chunk_text),\n",
        "            **doc[\"metadata\"]\n",
        "        })\n",
        "\n",
        "print(f\"Created {len(all_chunks)} chunks from {len(normalized_docs)} documents\")\n",
        "print(f\"\\nChunk token distribution:\")\n",
        "token_counts = [c[\"token_count\"] for c in all_chunks]\n",
        "print(f\"  Min: {min(token_counts)}, Max: {max(token_counts)}, Avg: {np.mean(token_counts):.1f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Embedding Generation\n",
        "\n",
        "Version embeddings and normalize vectors:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding configuration - version this with your index!\n",
        "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
        "EMBEDDING_VERSION = \"2025-01-01\"\n",
        "\n",
        "print(f\"Loading embedding model: {EMBEDDING_MODEL}\")\n",
        "embedder = SentenceTransformer(EMBEDDING_MODEL)\n",
        "\n",
        "def embed_batch(texts: List[str]) -> np.ndarray:\n",
        "    \"\"\"Embed texts with normalization for cosine similarity.\"\"\"\n",
        "    vectors = embedder.encode(texts, show_progress_bar=False)\n",
        "    vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
        "    return vectors\n",
        "\n",
        "# Generate embeddings\n",
        "chunk_texts = [c[\"text\"] for c in all_chunks]\n",
        "chunk_embeddings = embed_batch(chunk_texts)\n",
        "\n",
        "print(f\"Generated {len(chunk_embeddings)} embeddings with dimension {chunk_embeddings.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build Vector Index with ChromaDB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create ephemeral Chroma client\n",
        "client = chromadb.EphemeralClient(Settings(anonymized_telemetry=False))\n",
        "\n",
        "collection = client.create_collection(\n",
        "    name=\"demo_rag\",\n",
        "    metadata={\n",
        "        \"embedding_model\": EMBEDDING_MODEL,\n",
        "        \"embedding_version\": EMBEDDING_VERSION,\n",
        "        \"hnsw:space\": \"cosine\"\n",
        "    }\n",
        ")\n",
        "\n",
        "collection.add(\n",
        "    ids=[c[\"id\"] for c in all_chunks],\n",
        "    documents=[c[\"text\"] for c in all_chunks],\n",
        "    metadatas=[{\"doc_id\": c[\"doc_id\"], \"title\": c[\"title\"]} for c in all_chunks],\n",
        "    embeddings=chunk_embeddings.tolist()\n",
        ")\n",
        "\n",
        "print(f\"Added {collection.count()} chunks to vector index\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Hybrid Retrieval\n",
        "\n",
        "Combine vector search with lexical scoring:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lexical_score(query: str, text: str) -> float:\n",
        "    \"\"\"Simple lexical overlap score for keyword-heavy queries.\"\"\"\n",
        "    q_terms = set(query.lower().split())\n",
        "    t_terms = set(text.lower().split())\n",
        "    overlap = len(q_terms & t_terms)\n",
        "    return overlap / (len(q_terms) + 1e-6)\n",
        "\n",
        "def hybrid_retrieve(query: str, top_k: int = 5, vector_weight: float = 0.7, lexical_weight: float = 0.3) -> List[Dict]:\n",
        "    \"\"\"Hybrid retrieval with score fusion: score = w_vec * s_vec + w_lexical * s_lexical\"\"\"\n",
        "    q_vec = embed_batch([query])[0].tolist()\n",
        "    results = collection.query(\n",
        "        query_embeddings=[q_vec],\n",
        "        n_results=top_k * 2,\n",
        "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
        "    )\n",
        "    \n",
        "    scored = []\n",
        "    for i in range(len(results[\"ids\"][0])):\n",
        "        doc_text = results[\"documents\"][0][i]\n",
        "        vec_score = 1 - (results[\"distances\"][0][i] / 2)\n",
        "        lex_score = lexical_score(query, doc_text)\n",
        "        combined = vector_weight * vec_score + lexical_weight * lex_score\n",
        "        \n",
        "        scored.append({\n",
        "            \"id\": results[\"ids\"][0][i],\n",
        "            \"doc_id\": results[\"metadatas\"][0][i][\"doc_id\"],\n",
        "            \"title\": results[\"metadatas\"][0][i][\"title\"],\n",
        "            \"text\": doc_text,\n",
        "            \"vector_score\": vec_score,\n",
        "            \"lexical_score\": lex_score,\n",
        "            \"combined_score\": combined,\n",
        "        })\n",
        "    \n",
        "    scored.sort(key=lambda x: x[\"combined_score\"], reverse=True)\n",
        "    return scored[:top_k]\n",
        "\n",
        "# Test hybrid retrieval\n",
        "test_query = \"what are the payment terms?\"\n",
        "results = hybrid_retrieve(test_query, top_k=3)\n",
        "\n",
        "print(f\"Query: '{test_query}'\\n\")\n",
        "for i, r in enumerate(results):\n",
        "    print(f\"{i+1}. [{r['doc_id']}] {r['title']}\")\n",
        "    print(f\"   Score: {r['combined_score']:.3f} (vec: {r['vector_score']:.3f}, lex: {r['lexical_score']:.3f})\")\n",
        "    print(f\"   Text: {r['text'][:100]}...\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Reranking\n",
        "\n",
        "Reranking is cost-effective accuracy applied to top candidates:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_rerank(query: str, candidates: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Simple reranking using embedding similarity.\"\"\"\n",
        "    if not candidates:\n",
        "        return []\n",
        "    \n",
        "    q_vec = embed_batch([query])[0]\n",
        "    reranked = []\n",
        "    \n",
        "    for c in candidates:\n",
        "        c_vec = embed_batch([c[\"text\"]])[0]\n",
        "        rerank_score = float(np.dot(q_vec, c_vec))\n",
        "        final_score = 0.4 * c[\"combined_score\"] + 0.6 * rerank_score\n",
        "        \n",
        "        reranked.append({**c, \"rerank_score\": rerank_score, \"final_score\": final_score})\n",
        "    \n",
        "    reranked.sort(key=lambda x: x[\"final_score\"], reverse=True)\n",
        "    return reranked\n",
        "\n",
        "# Test reranking\n",
        "test_query = \"how long is customer data retained?\"\n",
        "candidates = hybrid_retrieve(test_query, top_k=5)\n",
        "reranked = simple_rerank(test_query, candidates)\n",
        "\n",
        "print(f\"Query: '{test_query}'\\n\")\n",
        "print(\"After reranking:\")\n",
        "for i, r in enumerate(reranked[:3]):\n",
        "    print(f\"{i+1}. [{r['doc_id']}] {r['title']}\")\n",
        "    print(f\"   Final: {r['final_score']:.3f} (rerank: {r['rerank_score']:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Context Assembly & Prompt Building\n",
        "\n",
        "Prepare context for LLM with source citations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_context(chunks: List[Dict], max_tokens: int = 2000) -> str:\n",
        "    \"\"\"Assemble context from chunks with headers.\"\"\"\n",
        "    blocks = []\n",
        "    total_tokens = 0\n",
        "    \n",
        "    for i, c in enumerate(chunks):\n",
        "        header = f\"[Source {i+1}] doc={c['doc_id']} | score: {c['final_score']:.2f}\"\n",
        "        if c.get(\"title\"):\n",
        "            header += f\"\\nTitle: {c['title']}\"\n",
        "        \n",
        "        block = f\"{header}\\n\\n{c['text']}\"\n",
        "        block_tokens = num_tokens(block)\n",
        "        \n",
        "        if total_tokens + block_tokens > max_tokens:\n",
        "            break\n",
        "        \n",
        "        blocks.append(block)\n",
        "        total_tokens += block_tokens\n",
        "    \n",
        "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
        "\n",
        "GROUNDED_PROMPT = \"\"\"You are an enterprise assistant. Use ONLY the context below to answer.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Instructions:\n",
        "- Answer based solely on the provided context\n",
        "- If the answer is not in the context, reply: \"Not found in context\"\n",
        "- Cite source numbers when making claims\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "# Build context and prompt\n",
        "context = build_context(reranked[:3])\n",
        "prompt = GROUNDED_PROMPT.format(context=context, query=test_query)\n",
        "\n",
        "print(f\"Context ({num_tokens(context)} tokens):\\n\")\n",
        "print(context[:500] + \"...\")\n",
        "print(f\"\\n\\nFull prompt: {num_tokens(prompt)} tokens\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Answer Verification\n",
        "\n",
        "Never trust a single LLM pass - verify deterministically:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def verify_answer(answer: str, context: str) -> Dict:\n",
        "    \"\"\"Level 1: Deterministic verification - check numbers exist in context.\"\"\"\n",
        "    issues = []\n",
        "    \n",
        "    answer_numbers = set(re.findall(r'\\b\\d+(?:\\.\\d+)?%?\\b', answer))\n",
        "    context_numbers = set(re.findall(r'\\b\\d+(?:\\.\\d+)?%?\\b', context))\n",
        "    \n",
        "    unsupported = answer_numbers - context_numbers\n",
        "    if unsupported:\n",
        "        issues.append(f\"Numbers not in context: {unsupported}\")\n",
        "    \n",
        "    return {\n",
        "        \"valid\": len(issues) == 0,\n",
        "        \"issues\": issues,\n",
        "        \"numbers_found\": list(answer_numbers & context_numbers)\n",
        "    }\n",
        "\n",
        "# Test verification\n",
        "test_answer = \"Customer data is retained for 7 years for audit purposes.\"\n",
        "verification = verify_answer(test_answer, context)\n",
        "\n",
        "print(f\"Answer: {test_answer}\")\n",
        "print(f\"Verification: {verification}\")\n",
        "\n",
        "# Test with hallucinated number\n",
        "bad_answer = \"Customer data is retained for 10 years.\"\n",
        "bad_verification = verify_answer(bad_answer, context)\n",
        "print(f\"\\nBad answer: {bad_answer}\")\n",
        "print(f\"Verification: {bad_verification}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Full RAG Pipeline\n",
        "\n",
        "Putting it all together:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rag_pipeline(query: str, top_k: int = 3, rerank: bool = True) -> Dict:\n",
        "    \"\"\"Complete RAG pipeline returning prompt and sources.\"\"\"\n",
        "    # 1. Retrieve\n",
        "    candidates = hybrid_retrieve(query, top_k=top_k * 2 if rerank else top_k)\n",
        "    \n",
        "    if not candidates:\n",
        "        return {\"prompt\": None, \"context\": \"\", \"sources\": [], \"error\": \"No relevant documents found\"}\n",
        "    \n",
        "    # 2. Rerank\n",
        "    if rerank:\n",
        "        candidates = simple_rerank(query, candidates)[:top_k]\n",
        "    \n",
        "    # 3. Build context\n",
        "    context = build_context(candidates)\n",
        "    \n",
        "    # 4. Build prompt\n",
        "    prompt = GROUNDED_PROMPT.format(context=context, query=query)\n",
        "    \n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"context\": context,\n",
        "        \"sources\": [{\"doc_id\": c[\"doc_id\"], \"title\": c[\"title\"], \"score\": c[\"final_score\"]} for c in candidates],\n",
        "        \"retrieved_ids\": [c[\"id\"] for c in candidates]\n",
        "    }\n",
        "\n",
        "# Run pipeline on multiple queries\n",
        "queries = [\n",
        "    \"What are the payment terms?\",\n",
        "    \"How long is data retained?\",\n",
        "    \"What are the API rate limits for enterprise?\",\n",
        "    \"What is the uptime SLA guarantee?\",\n",
        "]\n",
        "\n",
        "for q in queries:\n",
        "    result = rag_pipeline(q)\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Query: {q}\")\n",
        "    print(f\"Sources: {[s['doc_id'] for s in result['sources']]}\")\n",
        "    print(f\"Prompt tokens: {num_tokens(result['prompt'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated key production RAG components:\n",
        "\n",
        "1. **Ingestion & Normalization** - Clean input beats clever retrieval\n",
        "2. **Semantic Chunking** - Determines what the retriever can find\n",
        "3. **Embedding with Versioning** - Version embeddings with your index\n",
        "4. **Hybrid Retrieval** - Vector + lexical for better recall\n",
        "5. **Reranking** - Cost-effective accuracy improvement\n",
        "6. **Context Assembly** - Budget-aware with citations\n",
        "7. **Grounded Prompts** - Prevent hallucinations\n",
        "8. **Verification** - Never trust a single LLM pass\n",
        "\n",
        "For production deployment, see the FastAPI service and Kubernetes manifests in the repository.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
